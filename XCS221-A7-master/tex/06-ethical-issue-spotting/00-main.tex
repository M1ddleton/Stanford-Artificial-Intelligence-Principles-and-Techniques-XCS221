\item {\bf Ethical Issue Spotting}

One of the goals of this course is to teach you how to tackle real-world problems with tools from AI.  But real-world problems have real-world consequences. Along with technical skills, an important skill every practitioner of AI needs to develop is an awareness of the ethical issues associated with AI. The purpose of this exercise is to practice spotting potential ethical concerns in applications of AI - even seemingly innocuous ones.\\

In this question, you will explore the ethics of four different real-world scenarios using the ethics guidelines produced by a machine learning research venue, the NeurIPS conference. The \href{https://neurips.cc/public/EthicsGuidelines}{NeurIPS Ethical Guidelines} list sixteen non-exhaustive concerns under Potential Negative Social Impacts and General Ethical Conduct (the numbered lists). For each scenario, you will write a potential negative impacts statement. To do so, you will first determine if the algorithm / dataset / technique could have a potential negative
social impact or violate general ethical conduct (again, the sixteen numbered items taken from the \href{https://neurips.cc/public/EthicsGuidelines}{NeurIPS Ethical Guidelines} page). If the scenario does violate ethical conduct or has potential negative social impacts, list one concern it violates and justify why you think that concern applies to the scenario. If you do \textbf{not} think the scenario has an ethical concern, explain how you came to that decision. 
Unlike earlier problems in the homework there are many possible good answers. If you can justify your answer, then you should feel confident that you have answered the question well.\\

Each of the scenarios is drawn from a real AI research paper. The ethics of AI research closely mirror the potential real-world consequences of deploying AI, and the lessons you’ll draw from this exercise will certainly be applicable to deploying AI at scale. As a note, you are \textbf{not} required to read the original papers, but we have linked to them in case they might be useful. Furthermore, you are welcome to respond to anything in the linked article that's not mentioned in the written scenario, but the scenarios as described here should provide enough detail to find at least one concern.\\

\textbf{What we expect:} A 2-5 sentence paragraph for each of the scenarios where you either A. identify at least one ethical concern from the \href{https://neurips.cc/public/EthicsGuidelines}{NeurIPS Ethical Guidelines} and justify why you think it applies, or B. state that you don’t think a concern exists and justify why that’s the case. Chosen scenarios may have anywhere from zero to multiple concerns that match, but you are only required to pick one concern (if it exists) and justify your decision accordingly. Furthermore, copy out and underline the ethical checklist item to which you are referring as part of your answer (i.e.: \underline{Severely damage the environment}). We have also included a citation in the example solution below, but you are not required to add citations to your response.\\

\textbf{Example Scenario:} You work for a U.S. hospital that has recently implemented a new intervention program that enrolls at-risk patients in programs to help address their chronic medical issues proactively before the patients end up in the hospital. The intervention program automatically identifies at-risk patients by predicting patients’ risk scores, which are measured in terms of healthcare costs. However, you notice that for a given risk score tier, the Black patients are considerably sicker when enrolled than white patients, even though their assigned illness risk score is identical. You manually re-assign patients’ risk scores based on their current symptoms and notice that the percentage of Black patients who would be enrolled has increased from 17\%  to over 45\% \footnote{\href{https://doi.org/10.1126/science.aax2342}{Obermeyer et al. Dissecting racial bias in an algorithm used to manage the health of populations. 2019.}}.\\

\textbf{Example Solution:} This algorithm has likely \underline{encoded, contains}, \underline{or potentially exacerbates bias against people} \underline{of a certain race or ethnicity} since the algorithm predicts healthcare costs. Because access to medical care in the U.S. is unequal, Black patients tend to have lower healthcare costs than their white counterparts \footnote{\href{https://www.nap.edu/catalog/10260/unequal-treatment-confronting-racial-and-ethnic-disparities-in-health-care}{Institue of Medicine of the National Academies. Unequal Treatment:
Confronting Racial and Ethnic Disparities in Health Care. 2003}}. Thus the algorithm will incorrectly predict that they are at lower risk.\\

\begin{enumerate}

  \input{06-ethical-issue-spotting/01-scenario-1}

  \input{06-ethical-issue-spotting/02-scenario-2}

  \input{06-ethical-issue-spotting/03-scenario-3}

  \input{06-ethical-issue-spotting/04-scenario-4}

\end{enumerate}
